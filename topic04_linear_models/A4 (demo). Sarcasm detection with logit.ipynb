{"cells":[{"metadata":{"_uuid":"3f6c2bfe6b2e26c92357e896a1511195d836956e"},"cell_type":"markdown","source":"<center>\n<img src=\"https://habrastorage.org/files/fd4/502/43d/fd450243dd604b81b9713213a247aa20.jpg\">\n    \n## [mlcourse.ai](https://mlcourse.ai) â€“ Open Machine Learning Course \nAuthor: [Yury Kashnitskiy](https://yorko.github.io) (@yorko). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license. Free use is permitted for any non-commercial purpose."},{"metadata":{"_uuid":"cb01ca96934e5c83a36a2308da9645b87a9c52a0"},"cell_type":"markdown","source":"## <center> Assignment 4. Sarcasm detection with logistic regression\n    \nWe'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n\nSarcasm detection is easy. \n<img src=\"https://habrastorage.org/webt/1f/0d/ta/1f0dtavsd14ncf17gbsy1cvoga4.jpeg\" />"},{"metadata":{"trusted":true,"_uuid":"23a833b42b3c214b5191dfdc2482f2f901118247"},"cell_type":"code","source":"!ls ../input/sarcasm/","execution_count":3,"outputs":[{"output_type":"stream","text":"test-balanced.csv    train-balanced-sarc.csv.gz\r\ntest-unbalanced.csv  train-balanced-sarcasm.csv\r\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"ffa03aec57ab6150f9bec0fa56cd3a5791a3e6f4"},"cell_type":"code","source":"# some necessary imports\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport string\nfrom wordcloud import STOPWORDS\nfrom collections import defaultdict\nfrom scipy.sparse import hstack","execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/html":"<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>","text/vnd.plotly.v1+html":"<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"b23e4fc7a1973d60e0c6da8bd60f3d921542a856"},"cell_type":"code","source":"train_df = pd.read_csv('../input/sarcasm/train-balanced-sarcasm.csv')","execution_count":5,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4dc7b3787afa46c7eb0d0e33b0c41ab9821c4a27"},"cell_type":"code","source":"train_df.head()","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"   label                        ...                                                             parent_comment\n0      0                        ...                          Yeah, I get that argument. At this point, I'd ...\n1      0                        ...                          The blazers and Mavericks (The wests 5 and 6 s...\n2      0                        ...                                                    They're favored to win.\n3      0                        ...                                                 deadass don't kill my buzz\n4      0                        ...                          Yep can confirm I saw the tool they use for th...\n\n[5 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>comment</th>\n      <th>author</th>\n      <th>subreddit</th>\n      <th>score</th>\n      <th>ups</th>\n      <th>downs</th>\n      <th>date</th>\n      <th>created_utc</th>\n      <th>parent_comment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NC and NH.</td>\n      <td>Trumpbart</td>\n      <td>politics</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-16 23:55:23</td>\n      <td>Yeah, I get that argument. At this point, I'd ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>You do know west teams play against west teams...</td>\n      <td>Shbshb906</td>\n      <td>nba</td>\n      <td>-4</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-11</td>\n      <td>2016-11-01 00:24:10</td>\n      <td>The blazers and Mavericks (The wests 5 and 6 s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>They were underdogs earlier today, but since G...</td>\n      <td>Creepeth</td>\n      <td>nfl</td>\n      <td>3</td>\n      <td>3</td>\n      <td>0</td>\n      <td>2016-09</td>\n      <td>2016-09-22 21:45:37</td>\n      <td>They're favored to win.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>This meme isn't funny none of the \"new york ni...</td>\n      <td>icebrotha</td>\n      <td>BlackPeopleTwitter</td>\n      <td>-8</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-10</td>\n      <td>2016-10-18 21:03:47</td>\n      <td>deadass don't kill my buzz</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>I could use one of those tools.</td>\n      <td>cush2push</td>\n      <td>MaddenUltimateTeam</td>\n      <td>6</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>2016-12</td>\n      <td>2016-12-30 17:00:13</td>\n      <td>Yep can confirm I saw the tool they use for th...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"_uuid":"0a7ed9557943806c6813ad59c3d5ebdb403ffd78"},"cell_type":"code","source":"train_df.info()","execution_count":7,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1010826 entries, 0 to 1010825\nData columns (total 10 columns):\nlabel             1010826 non-null int64\ncomment           1010773 non-null object\nauthor            1010826 non-null object\nsubreddit         1010826 non-null object\nscore             1010826 non-null int64\nups               1010826 non-null int64\ndowns             1010826 non-null int64\ndate              1010826 non-null object\ncreated_utc       1010826 non-null object\nparent_comment    1010826 non-null object\ndtypes: int64(4), object(6)\nmemory usage: 77.1+ MB\n","name":"stdout"}]},{"metadata":{"_uuid":"6472f52fb5ecb8bb2a6e3b292678a2042fcfe34c"},"cell_type":"markdown","source":"Some comments are missing, so we drop the corresponding rows."},{"metadata":{"trusted":true,"_uuid":"97b2d85627fcde52a506dbdd55d4d6e4c87d3f08"},"cell_type":"code","source":"train_df.dropna(subset=['comment'], inplace=True)","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"9d51637ee70dca7693737ad0da1dbb8c6ce9230b"},"cell_type":"markdown","source":"We notice that the dataset is indeed balanced"},{"metadata":{"trusted":true,"_uuid":"addd77c640423d30fd146c8d3a012d3c14481e11"},"cell_type":"code","source":"train_df['label'].value_counts()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"0    505405\n1    505368\nName: label, dtype: int64"},"metadata":{}}]},{"metadata":{"_uuid":"5b836574e5093c5eb2e9063fefe1c8d198dcba79"},"cell_type":"markdown","source":"We split data into training and validation parts."},{"metadata":{"trusted":true,"_uuid":"c200add4e1dcbaa75164bbcc73b9c12ecb863c96"},"cell_type":"code","source":"train_texts, valid_texts, y_train, y_valid = \\\n        train_test_split(train_df['comment'], train_df['label'], random_state=17)","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_texts.head(), y_train.head()","execution_count":9,"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"(827869                      Should have named it Samsquanch\n 800568                       All that knob wants is uranus.\n 506459                  No their dogs gave up halfway here.\n 372707    I'm sure icefrog is that bad at critical self-...\n 548483      Thanks for your contribution to the discussion.\n Name: comment, dtype: object, 827869    0\n 800568    1\n 506459    0\n 372707    1\n 548483    1\n Name: label, dtype: int64)"},"metadata":{}}]},{"metadata":{"trusted":false,"_uuid":"7f0f47b98e49a185cd5cffe19fcbe28409bf00c0"},"cell_type":"markdown","source":"## Tasks:\n1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n\n## Links:\n  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions"},{"metadata":{},"cell_type":"markdown","source":"\nTask#1. Analyze the dataset, make some plots\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"# target count\ntarget_count = y_train.value_counts()\ntrace = go.Bar(x=target_count.index, y=target_count.values, marker=dict(color=target_count.values, colorscale='Picnic', reversescale=True))\n\nlayout = go.Layout(title='Target count', font=dict(size=18))\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='TargetCount')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# target distribution\nlabels = np.array(target_count.index)\nsizes = np.array((target_count / target_count.sum())*100)\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(title='Target distribution', font=dict(size=18), width=600, height=600)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='usertype')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# word frequency plot for sarcasmic and not comments\ntrain0_df = train_df[train_df['label']==0]\ntrain1_df = train_df[train_df['label']==1]\n\n# generate ngrams\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]\n\n# horizontal chart\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df['word'].values[::-1], \n        x=df['wordcount'].values[::-1], \n        showlegend=False, \n        orientation='h', \n        marker=dict(color=color))\n    return trace\n\n# bar chart for non sarcastic\nfreq_dict = defaultdict(int)\nfor sent in train0_df['comment']:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfreq_dict_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfreq_dict_sorted.columns = ['word', 'wordcount']\ntrace0 = horizontal_bar_chart(freq_dict_sorted.head(50), 'blue')\n\n# bar chart for sarcastic\nfreq_dict = defaultdict(int)\nfor sent in train1_df['comment']:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfreq_dict_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfreq_dict_sorted.columns = ['word', 'wordcount']\ntrace1 = horizontal_bar_chart(freq_dict_sorted.head(50), 'blue')\n\n# create two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, \n                         subplot_titles=['Freq words in non sarcastic comments', \n                                         'Freq words in sarcastic comments'])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title='Word Count Plots')\npy.iplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Observations:\n- Some of the top words are common across both the classes like one, people, will\n- The other top words in non sarcastic comments: think, fuck, good\n- The other top words in sarcastic comments: yeah, well, sure\n\nCreate bigrams frequency plots for both classes"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bar chart for non sarcastic\nfreq_dict = defaultdict(int)\nfor sent in train0_df['comment']:\n    for word in generate_ngrams(sent, 2):\n        freq_dict[word] += 1\nfreq_dict_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfreq_dict_sorted.columns = ['word', 'wordcount']\ntrace0 = horizontal_bar_chart(freq_dict_sorted.head(50), 'orange')\n\n# bar chart for sarcastic\nfreq_dict = defaultdict(int)\nfor sent in train1_df['comment']:\n    for word in generate_ngrams(sent, 2):\n        freq_dict[word] += 1\nfreq_dict_sorted_1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfreq_dict_sorted_1.columns = ['word', 'wordcount']\ntrace1 = horizontal_bar_chart(freq_dict_sorted_1.head(50), 'orange')\n\n# create two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.15,\n                         subplot_titles=['Freq bigrams in non sarcastic comments', \n                                         'Freq bigrams in sarcastic comments'])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title='Word Count Plots')\npy.iplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Observation:\n- non sarcastic comment often contains bigrams: pretty sure, pretty much, fake news and many junk comment with repeated words like jerry or fuck\n- sarcastic comment often contains bigrams: good thing, everyone knows, pretty sure, white people, black people\n\nCreate trigram plots"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bar chart for non sarcastic\nfreq_dict = defaultdict(int)\nfor sent in train0_df['comment']:\n    for word in generate_ngrams(sent, 3):\n        freq_dict[word] += 1\nfreq_dict_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfreq_dict_sorted.columns = ['word', 'wordcount']\ntrace0 = horizontal_bar_chart(freq_dict_sorted.head(50), 'green')\n\n# bar chart for sarcastic\nfreq_dict = defaultdict(int)\nfor sent in train1_df['comment']:\n    for word in generate_ngrams(sent, 3):\n        freq_dict[word] += 1\nfreq_dict_sorted_1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfreq_dict_sorted_1.columns = ['word', 'wordcount']\ntrace1 = horizontal_bar_chart(freq_dict_sorted_1.head(50), 'green')\n\n# create two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.15,\n                         subplot_titles=['Freq trigrams in non sarcastic comments', \n                                         'Freq trigrams in sarcastic comments'])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title='Word Count Plots')\npy.iplot(fig, filename='word-plots')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Observations:\n- many copy pasted words in top of the non sarcastic comments\n- number of trigrams in sarcastic comments is less and more adequate then non sarcastic\n\nCreate meta features:\n1. Number of words in the text\n2. Number of unique words in the text\n3. Number of characters in the text\n4. Number of stopwords\n5. Number of punctuations\n6. Number of upper case words\n7. Number of title case words\n8. Average length of the words\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_2 = pd.concat([train_texts, y_train], axis=1)\ntest_df_2 = pd.concat([valid_texts, y_valid], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_2.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_2.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = test_df_2['comment'][240293]\nprint(x)\nprint(str(x).split())\nprint([w for w in str(x).split() if w.istitle()])\nprint(len([w for w in str(x).split() if w.istitle()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n\n# Number of words in the text\ntrain_df_2['num_words'] = train_df_2['comment'].apply(lambda x: len(str(x).split()))\ntest_df_2['num_words'] = test_df_2['comment'].apply(lambda x: len(str(x).split()))\n\n# Number of unique words in the text\ntrain_df_2['num_unique_words'] = train_df_2['comment'].apply(lambda x: len(set(str(x).split())))\ntest_df_2['num_unique_words'] = test_df_2['comment'].apply(lambda x: len(set(str(x).split())))\n\n# Number of characters in the text\ntrain_df_2['num_chars'] = train_df_2['comment'].apply(lambda x: len(str(x)))\ntest_df_2['num_chars'] = test_df_2['comment'].apply(lambda x: len(str(x)))\n\n# Number of stopwords\ntrain_df_2['num_stopwords'] = train_df_2['comment'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df_2['num_stopwords'] = test_df_2['comment'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# Number of punctuations\ntrain_df_2['num_punctuations'] = train_df_2['comment'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\ntest_df_2['num_punctuations'] = test_df_2['comment'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# Number of upper case words\ntrain_df_2['num_words_upper'] = train_df_2['comment'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df_2['num_words_upper'] = test_df_2['comment'].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n# Number of title case words\ntrain_df_2['num_words_title'] = train_df_2['comment'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df_2['num_words_title'] = test_df_2['comment'].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n# Average length of the word\ntrain_df_2['mean_word_len'] = train_df_2['comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df_2['mean_word_len'] = test_df_2['comment'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Truncate some extreme values for better visulas\ntrain_df_2['num_words'].loc[train_df_2['num_words']>60] = 60\ntrain_df_2['num_punctuations'].loc[train_df_2['num_punctuations']>10] = 10\ntrain_df_2['num_chars'].loc[train_df_2['num_chars']>350] = 350","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_2.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"f, axes = plt.subplots(3, 1, figsize=(10, 20))\n\nsns.set(style='dark')\nsns.boxplot(x='label', y='num_words', data=train_df_2, ax=axes[0])\naxes[0].set_xlabel('Label', fontsize=12)\naxes[0].set_title('Number of words in each class', fontsize=15, )\n\nsns.boxplot(x='label', y='num_chars', data=train_df_2, ax=axes[1])\naxes[1].set_xlabel('Label', fontsize=12)\naxes[1].set_title('Number of characters in each class', fontsize=15)\n\nsns.boxplot(x='label', y='num_punctuations', data=train_df_2, ax=axes[2])\naxes[2].set_xlabel('Label', fontsize=12)\naxes[2].set_title('Number of punctuations in each class', fontsize=15)\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"Observations:\n- sarcastic comments have more number characters and words compared to non sarcastic comments\n- both classes have equally number punctuations"},{"metadata":{},"cell_type":"markdown","source":"### Task#2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (label) based on the text of a comment on Reddit (comment)."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time\n# create pipeline\ntfidf_logit_pipe = make_pipeline(TfidfVectorizer(ngram_range=(1,3), max_features=50000, min_df=2), \n                                 LogisticRegression(solver='lbfgs', n_jobs=4, random_state=17, max_iter=1000, verbose=1))\ntfidf_logit_pipe.fit(train_texts, y_train)\nprint(tfidf_logit_pipe.score(valid_texts, y_valid))\n# ngram_range=(1, 2), Wall time: 2min 5s, score=0.721196387725866\n# ngram_range=(1, 3), Wall time: 2min 3s, score=0.7218256072562071\n# ngram_range=(1, 4), Wall time: 3min 14s, score=0.7216989718790315","execution_count":11,"outputs":[{"output_type":"stream","text":"[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done   1 out of   1 | elapsed:   48.5s finished\n","name":"stderr"},{"output_type":"stream","text":"0.7218256072562071\nCPU times: user 1min 47s, sys: 4.07 s, total: 1min 51s\nWall time: 2min 40s\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"### Task#3. Plot the words/bigrams which a most predictive of sarcasm (you can use eli5 for that)"},{"metadata":{"trusted":true},"cell_type":"code","source":"import eli5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eli5.show_weights(estimator=tfidf_logit_pipe, top=100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Task#4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_subreddits, valid_subreddits = train_test_split(train_df['subreddit'], random_state=17)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create separate Tf-Idf vectorizers for comments and for subreddits. \n# It's possible to stick to a pipeline as well, but in that case it becomes a bit less straightforward (https://stackoverflow.com/questions/36731813/computing-separate-tfidf-scores-for-two-different-columns-using-sklearn)\n\ntf_idf_texts = TfidfVectorizer(ngram_range=(1, 3), max_features=50000, min_df=2)\ntf_idf_subreddits = TfidfVectorizer(ngram_range=(1, 1))","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# do transformations separately for comments and subreddits\nX_train_texts = tf_idf_texts.fit_transform(train_texts)\nX_valid_texts = tf_idf_texts.transform(valid_texts)\n\nX_train_subreddits = tf_idf_texts.fit_transform(train_subreddits)\nX_valid_subreddits = tf_idf_texts.transform(valid_subreddits)","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_texts.shape, X_valid_texts.shape, X_train_subreddits.shape, X_valid_subreddits.shape","execution_count":15,"outputs":[{"output_type":"execute_result","execution_count":15,"data":{"text/plain":"((758079, 50000), (252694, 50000), (758079, 7809), (252694, 7809))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# stack all features together\nX_train = hstack([X_train_texts, X_train_subreddits])\nX_valid = hstack([X_valid_texts, X_valid_subreddits])","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train.shape, X_valid.shape","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"((758079, 57809), (252694, 57809))"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit = LogisticRegression(C=1, n_jobs=4, random_state=17, verbose=1)","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit.fit(X_train, y_train)","execution_count":19,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='warn', n_jobs=4,\n          penalty='l2', random_state=17, solver='warn', tol=0.0001,\n          verbose=1, warm_start=False)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit.score(X_valid, y_valid)\n# C=1, score=0.727211568141705","execution_count":20,"outputs":[{"output_type":"execute_result","execution_count":20,"data":{"text/plain":"0.727211568141705"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"logit.get_params().keys()","execution_count":25,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"dict_keys(['C', 'class_weight', 'dual', 'fit_intercept', 'intercept_scaling', 'max_iter', 'multi_class', 'n_jobs', 'penalty', 'random_state', 'solver', 'tol', 'verbose', 'warm_start'])"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%time \n# find optimal value for L2 regularization hyperparameter C\nC_values_1 = [.001, .01, .1, 1, 10, 100, 1000] # for initial search\nC_values_2 = np.logspace(.1, 10, 10) # for fine tuning\nparam_grid_logit = {'C': C_values_2}\ngrid_logit = GridSearchCV(logit, param_grid_logit, return_train_score=True, cv=3, verbose=1)\n\ngrid_logit.fit(X_train, y_train)\n\n# with C_values_2 , Wall time: 7h 24min!!!, 'C': 1.2589254117941673","execution_count":21,"outputs":[{"output_type":"stream","text":"Fitting 3 folds for each of 10 candidates, totalling 30 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning:\n\nDefault solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 443.1min finished\n/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:1300: UserWarning:\n\n'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 4.\n\n","name":"stderr"},{"output_type":"stream","text":"[LibLinear]CPU times: user 7h 23min 44s, sys: 8.3 s, total: 7h 23min 52s\nWall time: 7h 24min\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print best value for hyperparameter C and cv_score\ngrid_logit.best_params_, grid_logit.best_score_","execution_count":22,"outputs":[{"output_type":"execute_result","execution_count":22,"data":{"text/plain":"({'C': 1.2589254117941673}, 0.7215989362586221)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for the validation set\ngrid_logit.score(X_valid, y_valid)","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"0.7268831076321559"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}